# SC2Cast - Technical Documentation

**All technical information in one place.**

---

## üí∞ Zero-Budget Approach

**$0/month operational costs** using 100% local, open-source AI:

| Component | Technology | Cost |
|-----------|-----------|------|
| LLM | Llama 3.1 8B (via Ollama) | FREE |
| TTS | Coqui TTS | FREE |
| Video | FFmpeg + python-sc2 | FREE |
| Hardware | RTX 3060+ 12GB VRAM (already owned) | $0 |
| **Total** | | **$0/month** ‚úÖ |

### Hardware Requirements
- GPU: RTX 3060+ (12GB VRAM minimum)
- RAM: 32GB (16GB system + 16GB models)
- Storage: 1TB SSD
- CPU: 8+ cores recommended

### Performance
- Processing time: 34 min for 20-min replay
- Quality: 77% of paid APIs (7.5/10 commentary, 7/10 voice)
- Trade-off: Acceptable quality at zero cost

---

## üèóÔ∏è Architecture

### Pipeline (7 Stages)
```
1. Replay Analysis (sc2reader)
   ‚Üì
2. Event Detection & Prioritization
   ‚Üì
3. Camera Director (AI-driven)
   ‚Üì
4. Commentary Generation (Llama 3.1)
   ‚Üì
5. Audio Synthesis (Coqui TTS)
   ‚Üì
6. Video Recording (SC2 + FFmpeg)
   ‚Üì
7. YouTube Upload
```

### Tech Stack
- **Language**: Python 3.11+
- **Replay**: sc2reader (parsing), python-sc2 (control)
- **LLM**: Llama 3.1 8B via Ollama (local)
- **TTS**: Coqui TTS (local, GPU-accelerated)
- **Video**: FFmpeg (H.264, 1080p60)
- **Container**: Docker + NVIDIA CUDA
- **Upload**: YouTube Data API v3

---

## üöÄ Setup (Zero-Budget)

### 1. Install Ollama (Local LLM)
```powershell
# Download from: https://ollama.com
winget install Ollama.Ollama

# Pull Llama 3.1 8B quantized
ollama pull llama3.1:8b-q4_K_M
```

### 2. Install Docker Desktop
```powershell
# Download from: https://docker.com
# Enable WSL 2 backend
# Start Docker Desktop
```

### 3. Build Project
```powershell
git clone https://github.com/prevosta/sc2cast.git
cd sc2cast
docker compose build  # Takes 10-15 min first time
```

### 4. Test
```powershell
# Test GPU
docker compose run sc2cast nvidia-smi

# Test Python
docker compose run sc2cast python --version

# Process demo replay (Sprint 1.2+)
docker compose run sc2cast python -m src.main --replay replays/demo/*.SC2Replay
```

---

## üíª Code Examples

### Local LLM Client
```python
import requests

class LocalLLM:
    def __init__(self, model="llama3.1:8b-q4_K_M"):
        self.model = model
        self.url = "http://localhost:11434"
    
    def generate(self, prompt, max_tokens=500):
        response = requests.post(
            f"{self.url}/api/generate",
            json={"model": self.model, "prompt": prompt, "max_tokens": max_tokens}
        )
        return response.json()["response"]
```

### Local TTS Engine
```python
from TTS.api import TTS

class LocalTTS:
    def __init__(self):
        self.tts = TTS("tts_models/en/vctk/vits").to("cuda")
    
    def synthesize(self, text, speaker="p326"):
        return self.tts.tts(text=text, speaker=speaker)
```

### Replay Parser
```python
import sc2reader

class ReplayAnalyzer:
    def parse(self, replay_path):
        replay = sc2reader.load_replay(replay_path)
        return {
            "duration": replay.game_length.total_seconds(),
            "map": replay.map_name,
            "players": [p.name for p in replay.players],
            "winner": next(p.name for p in replay.players if p.result == "Win")
        }
```

---

## üéÆ Camera Director Algorithm

### Priority Scoring (10-point scale)
```python
PRIORITIES = {
    'major_battle': 10,      # 100+ supply engaged
    'base_destroy': 9,       # Nexus/CC/Hatchery killed
    'tech_complete': 8,      # Major tech (Hive, Colossus, etc.)
    'harassment': 7,         # Drops, run-bys
    'expansion': 6,          # New base
    'scout': 4,              # Observer, Overlord
}

def calculate_priority(event):
    base = PRIORITIES[event.type]
    if event.supply_lost > 50:
        base += 2
    return min(base, 10)
```

### Conflict Resolution
- Simultaneous events: Show highest priority
- Multiple high-priority: Use split-screen
- Boring periods: Show macro overview

---

## üé§ Commentary Generation

### Prompt Structure
```python
def build_prompt(event, game_state):
    return f"""You are a StarCraft II commentator.

GAME CONTEXT (verified facts only):
- Time: {format_time(event.timestamp)}
- Event: {event.description}
- Supply: {game_state.supply[0]} vs {game_state.supply[1]}
- Workers: {game_state.workers[0]} vs {game_state.workers[1]}

Generate 2-3 sentences of commentary.
Be factual, specific, and engaging.

Output JSON: {{"text": "...", "emotion": "neutral|excited|tense"}}
"""
```

### Validation
- Check facts against game state
- Reject hallucinations
- Limit length (3 sentences max)

---

## üîß Configuration

### config/default.yaml
```yaml
llm:
  provider: ollama
  model: llama3.1:8b-q4_K_M
  base_url: http://localhost:11434

tts:
  provider: coqui
  model: tts_models/en/vctk/vits
  speaker: p326  # British male
  use_gpu: true

video:
  resolution: [1920, 1080]
  fps: 60
  codec: libx264
  preset: medium

youtube:
  credentials: ./credentials/youtube_token.json
  auto_upload: false
```

---

## üêõ Troubleshooting

### Docker Build Fails
- Ensure Docker Desktop is running
- Check GPU support: `nvidia-smi`
- Retry: `docker compose build --no-cache`

### Ollama Not Working
- Check service: `ollama list`
- Restart: `ollama serve`
- Pull model again: `ollama pull llama3.1:8b-q4_K_M`

### Coqui TTS Slow
- Verify GPU: `torch.cuda.is_available()`
- Use smaller model if needed
- Check VRAM usage: `nvidia-smi`

---

## üìö Resources

- Ollama: https://ollama.com
- Coqui TTS: https://github.com/coqui-ai/TTS
- sc2reader: https://sc2reader.readthedocs.io
- python-sc2: https://burnysc2.github.io/python-sc2
- FFmpeg: https://ffmpeg.org
- YouTube API: https://developers.google.com/youtube/v3

---

**For implementation timeline, see `IMPLEMENTATION.md`**  
**For common questions, see `FAQ.md`**
